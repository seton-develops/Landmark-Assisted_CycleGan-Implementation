# Landmark-Assisted-StarGAN
Project to convert images from CelebA dataset to bitemoji avatars.


The CycleGAN implementation is inspired by the official Pytorch implementation of CycleGAN: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix ,

and aladdinpersson's simple CycleGAN implementation: https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/CycleGAN.

We trained the StarGAN with our processed dataset using the pre-trained StarGAN model from the official Pytorch implementation of StarGAN: https://github.com/yunjey/stargan

Our dataset for human and cartoon were from CelebA: https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html, and Bitmoji Faces: https://www.kaggle.com/datasets/mostafamozafari/bitmoji-faces.

Clone this repo:

`git clone https://github.com/seton-develops/ECE228_FINAL_PROJECT_GROUP24.git`


## Set up your environment

To ensure that you can run our model without any error, you should create a conda environment using the provided `.yml` file. 

Run:

`conda env create -f environment.yml`

This might take up to 5 minutes. 

## Overview
IMPORTANT: PLEASE DELETE ANY FILE INSIDE `~/trainA`, `~/trainB`, `~/valA`, `~/valB`, and `~/saved_images` folder. Since Github does not allow uploading empty folder, we put an empty txt file there. 

Inside the `Landmark-Assisted-StarGAN` folder:

`~/data` contains the training, testing data, and the landmark coordinate xlsx files we provided for both domains.

`~/saved_images` contains the images generated during the training process.

`~/stargan_input` contains the selected input images from CelebA dataset for testing purposes.

`~/stargan_output` contains the selected output images generated by the pre-trained StarGAN model for testing purposes.

`Testing_visualization.ipynb` is for testing the model you trained or the pre-trained model we provided.

`cartoon_torch.pt` is the pre-trained regressor model for Bitmoji dataset.

`config.py` contains the hyper-parameters, number of epochs, options for load/save models, and etc. You can always change the values to fine-tune. 

`dataset.py` is the dataloader we created specificly for this task.

`human_torch.pt` is the pre-trained regressor model for CelebA dataset.

`image_preprocessing_FINAL.ipynb` is the notebook for image preprocessing. NOTE: IT IS NOT necessary to run this in order to train the model. The path was set to our local device. 

`local_landmark_discriminator.py` is the local landmark discriminator model used to detect whether the eyes, nose, and mouth are real.

`model_D.py` is the discriminator model from https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/CycleGAN.

`model_G.py` is the generator model from https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/CycleGAN.

`model_R.py` is the regressor model based on the TCDCN architecture for later input into the model we returned from `local landmark discriminator.py`.

`patch_extractor.py` contains functions that crop the images into patches.

`train.py` is the SECOND training stage set up for our model (CycleGAN + landmark consistency loss + local discriminator). 

`train_first.py` is the FIRST training set up for our model (CycleGAN + landmark consistency loss).

`utils.py` contains the utils functions to save/load models and calculate the landmark consistency loss.

## Download dataset or download our pre-trained models

https://drive.google.com/drive/folders/1b2HdQjdiX-RCQDwpKEbx4qdLChmZARzO?usp=sharing

### dataset
Download `data.zip` in the Google Drive link we provided above.
Unzip it and replace the `~/Landmark-Assisted-StarGAN/data` folder. 

The `~/data` folder should have the following structure:   

```bash
data
   ├── train
   │       ├── trainA
   │       │        └─── *.jpg
   │       ├── trainB
   │       │        └─── *.jpg
   │       ├── trainA_human_landmarks.xlsx
   │       └─── trainB_cartoon_landmarks.xlsx
   │
   └─── val
          ├── valA
          │      └─── *.jpg
          ├── valB
          │      └─── *.jpg
          ├── valA_human_landmarks.xlsx
          └─── valB_cartoon_landmarks.xlsx

```


If you just replace the `~/data` folder, there should not be any problem with the folder structure.            

NOTE: we did not use the val data for validation because it is a GAN model. The data in `~/valA(B)` folder is for testing.

### pre-trained models
If you want to use the pre-trained models, download `pre-trained-models.zip` in the same Google Drive link.
Please put evey pre-trained model inside the `~/Landmark-Assisted-StarGAN` folder. You should have total of 10 `.pth.tar` files.

In fact, you only need the generator model `G_H.pth.tar` to test and visualize, but we provided all the models anyway so you can also continue training or use them to your demand. 



## Training the model
IMPORTANT: GO TO `config.py` FIRST. Change `TRAIN_DIR` and `VAL_DIR` to absolute path, for example : `/home/zluan/ECE228_FINAL_PROJECT_GROUP24-main/Landmark-Assisted-StarGAN/data/train`. (relative path might lead to "[Errno 2] No such file or directory:")

If you want to use a different training setting, feel free to change the hyper-parameters in `config.py`.

If you want to recreate our training process, run:

`cd ECE228_FINAL_PROJECT_GROUP24/Landmark-Assisted-StarGAN`

Then, run:

`python train_first.py`

The images generated during the training process is saved in `saved_images` folder. 

After training with 60 epochs, feel free to test and visualize this model at this stage. Please refer to `Test and visualize the model` section. 


If you want to continue, you need to go to `config.py` and change the `NUM_EPOCHS` to 40 and change `LOAD_MODEL` to True.

Then, run:

`python train.py`

## Test and visualize the model


Go to `Testing_visualization.ipynb` to test and visualize the result. 
We provided 20 images selected from CelebA dataset and 20 images generated from the pre-trained StarGAN model inside the stargan_input and stargan_out folder.

If you set up everything correctly, you should be able to run the cell in sequence without any problem.




## Live demo

We created a live demo for our model using local webcam. If you have the device you can try out this live demo yourself (It has to be ran locally). 

You can download the `model_demo.zip` to your local device. Make sure to change the path to your ABSOLUTE PATH to `demo_pictures`.

https://drive.google.com/drive/folders/1b2HdQjdiX-RCQDwpKEbx4qdLChmZARzO?usp=sharing

If you want to try it yourself, you can click this link and go to `videos of live demonstration` section and check out our tutorial videos. 

If you do not have the device (webcam), or do not want to try the live demo, feel free to watch our videos to see how we did it.  

In the last cell of the `Demo.ipynb` :

Make sure the values in these two lines are 320, and 180

`cam.set(cv2.CAP_PROP_FRAME_WIDTH, 320)`

`cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 180)`

